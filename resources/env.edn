#mmerge
 [{;; Configuration for the LLM services. See '#include "config.edn"' at the
   ;; bottom for secrets and other local props. Whatever is declared in `config.edn`
   ;; will override the values declared here.

   ;; ######################################################################
   ;; LLM services
   ;; ######################################################################

   :openai       {:api-endpoint #or [#env "OPENAI_API_ENDPOINT" "https://api.openai.com/v1"]
                  :model-params {:model :gpt-3.5-turbo}
                  :complete-fn  bosquet.llm.openai/complete
                  :chat-fn      bosquet.llm.openai/chat
                  :impl         :openai
                  ;; A list of model names supported by this service. It is an
                  ;; optional data point that allows a shortcut when defining LLM
                  ;; calls with (generator/llm) function. Instead of
                  ;; `(llm :openai :model-params {:model :gpt-3.5})`
                  ;; a shorthand of `(llm :gpt-3.5)` will work
                  :model-names
                  #{:babbage-002 :davinci-002
                    :gpt-3.5 :gpt-3.5-turbo
                    :gpt-3.5-turbo-0125 :gpt-3.5-turbo-0301
                    :gpt-3.5-turbo-0613 :gpt-3.5-turbo-1106
                    :gpt-3.5-turbo-16k-0613 :gpt-3.5-turbo-instruct
                    :gpt-4 :gpt-4-0125-preview :gpt-4-1106-preview
                    :gpt-4-1106-vision-preview :gpt-4-32k
                    :gpt-4o :gpt-4o-2024-05-13
                    ;; embeddings
                    :text-embedding-3-large
                    :text-embedding-3-small
                    :text-embedding-ada-002}}
   :openai-azure {:api-endpoint #env "AZURE_OPENAI_API_ENDPOINT"
                  :model-params {:model :gpt-3.5-turbo}
                  :impl         :azure}
   :ollama       {:api-endpoint #or [#env "OLLAMA_API_ENDPOINT" "http://localhost:11434/api"]
                  :complete-fn  bosquet.llm.ollama/complete
                  :chat-fn      bosquet.llm.ollama/chat
                  :embed-fn     bosquet.llm.ollama/create-embedding}
   :lmstudio     {:api-endpoint #or [#env "LMSTUDIO_API_ENDPOINT" "http://localhost:1234/v1"]
                  :complete-fn  bosquet.llm.oai-shaped-llm/complete
                  :chat-fn      bosquet.llm.oai-shaped-llm/chat}
   :mistral      {:api-endpoint #or [#env "MISTRAL_API_ENDPOINT" "https://api.mistral.ai/v1"]
                  :model-params {:model       :mistral-small-latest
                                 :temperature 0}
                  :complete-fn  bosquet.llm.oai-shaped-llm/complete
                  :chat-fn      bosquet.llm.oai-shaped-llm/chat
                  :model-names
                  #{:open-mistral-7b :open-mixtral-8x7b
                    :mistral-small-latest :mistral-medium-latest :mistral-large-latest
                    :mistral-small :mistral-medium :mistral-large
                    :open-mistral-nemo
                    :mistral-embed
                    :open-codestral-mamba :codestral-latest}}
   :claude       {:api-endpoint #or [#env "CLAUDE_API_ENDPOINT" "https://api.anthropic.com/v1"]
                  :model-params          {:model       :claude-3-haiku-20240307
                                          ;; max tokens is required parameter when
                                          ;; making a call to Claude, use this as default
                                          :max_tokens  500
                                          :temperature 0.6}
                  :chat-fn               bosquet.llm.claude/messages
                  :model-names
                  #{:claude-3-5-sonnet-20240620 :claude-3-opus-20240229 :claude-3-sonnet-20240229
                    :claude-3-haiku-20240307}}
   :cohere       {:model-params {:model       :command
                                 :temperature 0}
                  :complete-fn  bosquet.llm.cohere/complete
                  :chat-fn      bosquet.llm.cohere/chat
                  :model-names
                  #{:command :command-light :command-light-nightly
                    :command-r-plus :command-r}}
   :llama-pplx   {:api-endpoint #or [#env "PPLX_API_ENDPOINT" "https://api.perplexity.ai"]
                  :api-key #env "PPLX_API_KEY"
                  :model-params {:model       :llama-3.1-sonar-small-128k-online}  
                  :complete-fn  bosquet.llm.oai-shaped-llm/complete
                  :chat-fn      bosquet.llm.oai-shaped-llm/chat
                  :model-names
                  #{:llama-3.1-sonar-small-128k-online
                    :llama-3.1-sonar-large-128k-online
                    :llama-3.1-sonar-huge-128k-online}}
 
   ;; ########
   ;; DB
   ;; ########

   :qdrant {:api-endpoint #or [#env "QDRANT_API_ENDPOINT" "http://localhost:6333"]
            :on-disk      true
            :size         1536
            :distance     :Dot}}

  ;; config.edn contains local settings for the LLM services, tools,
  ;; and other components, values in this file will override the above
  #include-config "config.edn"
  #include-config "secrets.edn"]
