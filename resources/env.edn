#mmerge
 [{;; Configuration for the LLM services. See '#include "config.edn"' at the
   ;; bottom for secrets and other local props. Whatever is declared in `config.edn`
   ;; will override the values declared here.

   ;; ######################################################################
   ;; LLM services
   ;; ######################################################################

   :openai {:api-endpoint #or [#env "OPENAI_API_ENDPOINT" "https://api.openai.com/v1"]
            :model-params {:model :gpt-3.5-turbo}
            :complete-fn  bosquet.llm.openai/complete
            :chat-fn      bosquet.llm.openai/chat
            :impl         :openai
                  ;; A list of model names supported by this service. It is an
                  ;; optional data point that allows a shortcut when defining LLM
                  ;; calls with (generator/llm) function. Instead of
                  ;; `(llm :openai :model-params {:model :gpt-3.5})`
                  ;; a shorthand of `(llm :gpt-3.5)` will work
            :model-names
            #{:babbage-002 :davinci-002
              :gpt-3.5 :gpt-3.5-turbo
              :gpt-3.5-turbo-0125 :gpt-3.5-turbo-0301
              :gpt-3.5-turbo-0613 :gpt-3.5-turbo-1106
              :gpt-3.5-turbo-16k-0613 :gpt-3.5-turbo-instruct
              :gpt-4 :gpt-4-0125-preview :gpt-4-1106-preview
              :gpt-4-1106-vision-preview :gpt-4-32k
              :gpt-4o :gpt-4o-2024-05-13
              :gpt-5-nano :gpt-5 :gpt-5-mini
              ;; embeddings
              :text-embedding-3-large
              :text-embedding-3-small
              :text-embedding-ada-002}}

   :localai {:api-endpoint #or [#env "LOCALAI_API_ENDPOINT" "http://0.0.0.0:8080/v1"]
             :model-params {:model :phi-4}
             :complete-fn  bosquet.llm.localai/complete
             :chat-fn      bosquet.llm.localai/chat
             :impl         :openai
             :model-names  #{:phi-4}}

   :openai-azure {:api-endpoint #env "AZURE_OPENAI_API_ENDPOINT"
                  :model-params {:model :gpt-3.5-turbo}
                  :impl         :azure}
   :ollama       {:api-endpoint #or [#env "OLLAMA_API_ENDPOINT" "http://localhost:11434/api"]
                  :complete-fn  bosquet.llm.ollama/complete
                  :chat-fn      bosquet.llm.ollama/chat
                  :embed-fn     bosquet.llm.ollama/create-embedding}
   :lmstudio     {:api-endpoint #or [#env "LMSTUDIO_API_ENDPOINT" "http://localhost:1234/v1"]
                  :complete-fn  bosquet.llm.oai-shaped-llm/complete
                  :chat-fn      bosquet.llm.oai-shaped-llm/chat
                  :model-params {:max_tokens  -1
                                 :temperature 0
                                 :stream      false}}
   :mistral      {:api-endpoint #or [#env "MISTRAL_API_ENDPOINT" "https://api.mistral.ai/v1"]
                  :model-params {:model       :mistral-small-latest
                                 :temperature 0}
                  :complete-fn  bosquet.llm.oai-shaped-llm/complete
                  :chat-fn      bosquet.llm.oai-shaped-llm/chat
                  :model-names
                  #{:open-mistral-7b :open-mixtral-8x7b
                    :mistral-small-latest :mistral-medium-latest :mistral-large-latest
                    :mistral-small :mistral-medium :mistral-large
                    :open-mistral-nemo
                    :mistral-embed
                    :open-codestral-mamba :codestral-latest}}
   :claude       {:api-endpoint #or [#env "CLAUDE_API_ENDPOINT" "https://api.anthropic.com/v1"]
                  :api-key      [#env "ANTHROPIC_API_KEY"]
                  :model-params {:model       :claude-3-5-haiku-latest
                                          ;; max tokens is required parameter when
                                          ;; making a call to Claude, use this as default
                                 :max_tokens  500
                                 :temperature 0.6}
                  :chat-fn      bosquet.llm.claude/messages
                  :model-names
                  #{:claude-3-opus-latest :claude-3-5-haiku-latest :claude-3-7-sonnet-latest
                    :claude-3-5-sonnet-latest
                    :claude-3-7-sonnet-20250219
                    :claude-3-5-haiku-20241022 :claude-3-5-sonnet-20241022
                    :claude-3-5-sonnet-20240620 :claude-3-opus-20240229 :claude-3-sonnet-20240229
                    :claude-3-haiku-20240307}}
   :cohere       {:model-params {:model       :command
                                 :temperature 0}
                  :complete-fn  bosquet.llm.cohere/complete
                  :chat-fn      bosquet.llm.cohere/chat
                  :model-names
                  #{:command :command-light :command-light-nightly
                    :command-r-plus :command-r}}
   :pplx         {:api-endpoint #or [#env "PPLX_API_ENDPOINT" "https://api.perplexity.ai"]
                  :api-key      #env "PPLX_API_KEY"
                  :model-params {:model :sonar}
                  :complete-fn  bosquet.llm.oai-shaped-llm/complete
                  :chat-fn      bosquet.llm.oai-shaped-llm/chat
                  :model-names
                  #{:llama-3.1-sonar-small-128k-online ;; will probably deprecate soon
                    :llama-3.1-sonar-large-128k-online ;; will probably deprecate soon  
                    :llama-3.1-sonar-huge-128k-online  ;; will probably deprecate soon
                    :sonar-deep-research
                    :sonar-reasoning-pro
                    :sonar-reasoning
                    :sonar
                    :sonar-pro
                    :r1-1776}}
   :groq         {:api-endpoint #or [#env "GROQ_API_ENDPOINT" "https://api.groq.com/openai/v1"]
                  :api-key      #env "GROQ_API_KEY"
                  :model-params {:model :llama-3.3-70b-versatile}
                  :complete-fn  bosquet.llm.oai-shaped-llm/complete
                  :chat-fn      bosquet.llm.oai-shaped-llm/chat
                  :model-names
                  #{:deepseek-r1-distill-llama-70b
                    :deepseek-r1-distill-qwen-32b
                    :gemma2-9b-it
                    :llama-3.1-8b-instant
                    :llama-3.2-1b-preview
                    :llama-3.2-3b-preview
                    :llama-3.3-70b-specdec
                    :llama-3.3-70b-versatile
                    :meta-llama/llama-4-scout-17b-16e-instruct
                    :meta-llama/llama-4-maverick-17b-128e-instruct
                    :llama-guard-3-8b
                    :llama3-8b-8192
                    :llama3-70b-8192
                    :mistral-saba-24b
                    :qwen-2.5-32b
                    :qwen-2.5-coder-32b
                    :qwen-qwq-32b
                    :allam-2-7b}}

   ;; ########
   ;; DB
   ;; ########

   :qdrant {:api-endpoint #or [#env "QDRANT_API_ENDPOINT" "http://localhost:6333"]
            :on-disk      true
            :size         1536
            :distance     :Dot}}

  ;; config.edn contains local settings for the LLM services, tools,
  ;; and other components, values in this file will override the above
  #include-config "config.edn"
  #include-config "secrets.edn"]
